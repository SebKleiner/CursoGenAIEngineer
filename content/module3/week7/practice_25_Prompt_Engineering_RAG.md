# Clase 25: Prompt Engineering RAG
Retrieval-augmented generation (RAG) es una arquitectura de software que combina la inteligencia artificial (IA) con fuentes de informaci√≥n externas.

![Arquitectura RAG](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/jumpstart/jumpstart-fm-rag.jpg)

Una aplicaci√≥n RAG t√≠pica tiene dos componentes principales:

- Indexaci√≥n: Una canalizaci√≥n para ingerir datos desde una fuente e indexarlos. Esto normalmente ocurre en modo offline (sin conexi√≥n).

![Indexaci√≥n](https://python.langchain.com/assets/images/rag_indexing-8160f90a90a33253d0154659cf7d453f.png)

- Recuperaci√≥n y generaci√≥n: La cadena RAG propiamente dicha, que toma la consulta del usuario en tiempo de ejecuci√≥n, recupera los datos relevantes del √≠ndice, y luego los env√≠a al modelo para generar la respuesta.

![Recuperaci√≥n y generaci√≥n](https://python.langchain.com/assets/images/rag_retrieval_generation-1046a4668d6bb08786ef73c56d4f228a.png)

## Ejercicio 1: Incorporaci√≥n de conocimiento externo

Con un sistema de recuperaci√≥n implementado, necesitamos transferir el conocimiento de este sistema al modelo. Un pipeline RAG t√≠picamente logra esto siguiendo estos pasos:

1. Recibir una consulta de entrada.
2. Utilizar el sistema de recuperaci√≥n para buscar informaci√≥n relevante basada en la consulta.
3. Incorporar la informaci√≥n recuperada en el prompt enviado al LLM.
4. Generar una respuesta que aproveche el contexto recuperado.

1. Instalar las dependencias
``` bash
pip install langchain-openai faiss-cpu langchain-text-splitters langchain-community python-dotenv
```

2. Generar un script (rag.py)
``` python
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.document_loaders import TextLoader
import os
from dotenv import load_dotenv

# Cargar variables de entorno AL INICIO
load_dotenv()

# 1. Cargar documentos (ejemplo)
loader = TextLoader("tu_archivo.txt")  # Reemplaza con tu fuente de datos
documents = loader.load()

# 2. Dividir documentos en chunks
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
texts = text_splitter.split_documents(documents)

# 3. Crear base vectorial y retriever
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_documents(texts, embeddings)
retriever = vectorstore.as_retriever()

# Resto del c√≥digo original (modificado para mejor pr√°ctica)
system_prompt = """Eres un asistente para tareas de pregunta-respuesta. 
Usa los siguientes fragmentos de contexto recuperado para responder. 
Si no sabes la respuesta, di que no lo sabes. 
Usa m√°ximo tres oraciones y mant√©n la respuesta concisa.
Contexto: {context}"""

question = "¬øCu√°les son los componentes principales de un sistema de agente aut√≥nomo impulsado por LLM?"

# Recuperar documentos relevantes
docs = retriever.invoke(question)  # Ahora funciona
docs_text = "\n".join(d.page_content for d in docs)

# Formatear prompt
formatted_prompt = system_prompt.format(context=docs_text)

# Crear y ejecutar modelo
model = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
response = model.invoke([
    SystemMessage(content=formatted_prompt),
    HumanMessage(content=question)
])

print(response.content)
```

3. Generar un archivo que se llame tu_archivo.txt y que contanga
```bash
Los componentes principales de un agente aut√≥nomo con LLM son:
- M√≥dulo de planificaci√≥n
- Memoria a largo plazo
- Sistema de recuperaci√≥n (RAG)
- Modelo de lenguaje (LLM)
- M√≥dulo de ejecuci√≥n de tareas
```

3. Ejecutar el script
```bash
python rag.py
```

Explicaci√≥n breve de los pasos:
- Paso 1: El usuario realiza una pregunta (ej: "¬øC√≥mo optimizar consultas SQL?").
- Paso 2: Se buscan fragmentos de documentos relevantes (ej: gu√≠as t√©cnicas sobre SQL).
- Paso 3: El contexto recuperado se inserta en el prompt (ej: "Responde usando esta informaci√≥n: [contexto]...").
- Paso 4: El LLM genera una respuesta precisa y contextualizada (ej: pasos espec√≠ficos para optimizaci√≥n).

** Ejercicio Extra**:
1. Ingresa nueva informaci√≥n a "tu_archivo.txt" y cambia la pregunta para que se pueda responder con la informaci√≥n nueva
2. Cambia la pregunta por una que no se pueda responder con la informaci√≥n que hay en  "tu_archivo.txt"

## Ejercicio 2: RAG con LangChain (Flujo Completo)

1. Instalar las dependencias
``` bash
pip install fastapi uvicorn langchain langchain-openai langchain-community python-dotenv langchain-huggingface langchain-pinecone
```

2. Estructura de archivos
```bash
‚îú‚îÄ‚îÄ .env            # Clave de OpenAI
‚îú‚îÄ‚îÄ main.py         # API con FastAPI
‚îî‚îÄ‚îÄ documents/      # Archivos subidos (opcional)
```

3. L√≥gica RAG (rag_handler.py)
``` python
import os
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

class OpenAIRAGHandler:
    def __init__(self):
        # Configurar embeddings y LLM de OpenAI
        self.embeddings = OpenAIEmbeddings()
        self.llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.3)
        self.vector_store = None
        
        # Plantilla de prompt
        self.prompt_template = PromptTemplate(
            template="""
            Eres un asistente experto. Responde la pregunta usando SOLO el contexto proporcionado.
            Si no hay informaci√≥n relevante, di 'No tengo datos suficientes'.
            
            Contexto: {context}
            
            Pregunta: {question}
            
            Respuesta:
            """,
            input_variables=["context", "question"]
        )

    def add_text(self, text: str):
        # Dividir texto en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_text(text)
        
        # Crear o actualizar FAISS
        if self.vector_store is None:
            self.vector_store = FAISS.from_texts(texts, self.embeddings)
        else:
            self.vector_store.add_texts(texts)

    def add_file(self, file_path: str):
        # Cargar documento seg√∫n tipo
        if file_path.endswith(".pdf"):
            loader = PyPDFLoader(file_path)
        else:
            loader = TextLoader(file_path)
        
        documents = loader.load()
        
        # Dividir y a√±adir
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        texts = text_splitter.split_documents(documents)
        
        if self.vector_store is None:
            self.vector_store = FAISS.from_documents(texts, self.embeddings)
        else:
            self.vector_store.add_documents(texts)

    def ask(self, question: str) -> str:
        if self.vector_store is None:
            return "Error: No hay documentos cargados."
        
        # Crear cadena QA
        qa_chain = RetrievalQA.from_chain_type(
            llm=self.llm,
            chain_type="stuff",
            retriever=self.vector_store.as_retriever(search_kwargs={"k": 3}),
            chain_type_kwargs={"prompt": self.prompt_template}
        )
        
        result = qa_chain.invoke({"query": question})
        return result["result"]
```

4. API con FastAPI (main.py)

```python
from fastapi import FastAPI, UploadFile, File, HTTPException
from pydantic import BaseModel
from rag_handler import OpenAIRAGHandler
import os

app = FastAPI()
rag = OpenAIRAGHandler()

class QueryRequest(BaseModel):
    question: str

class AddTextRequest(BaseModel):
    text: str

@app.post("/add-text")
async def add_text(request: AddTextRequest):
    try:
        rag.add_text(request.text)
        return {"status": "Texto a√±adido exitosamente"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/add-file")
async def add_file(file: UploadFile = File(...)):
    try:
        # Guardar archivo temporalmente
        file_path = f"documents/{file.filename}"
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        # Procesar archivo
        rag.add_file(file_path)
        return {"status": f"Archivo {file.filename} procesado"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    finally:
        # Limpiar archivo temporal
        if os.path.exists(file_path):
            os.remove(file_path)

@app.post("/ask")
async def ask_question(request: QueryRequest):
    try:
        answer = rag.ask(request.question)
        return {"question": request.question, "answer": answer}
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

5. Ejecutar la API
```bash
uvicorn main:app --reload
```

# ¬øPor qu√© usar RAG en lugar de "poner un documento" directamente al LLM?

## Ejemplo: Pregunta sobre un Documento Largo
Imagina que tienes un **manual t√©cnico de 200 p√°ginas** y quieres hacer preguntas espec√≠ficas sobre su contenido.

---

### **Opci√≥n 1: Enviar el documento "tal cual" al LLM (sin RAG)**
- **Proceso**:  
  Pegas todo el texto del manual en el prompt del LLM y preguntas:  
  `"¬øC√≥mo reiniciar el servidor seg√∫n el manual?"`

- **Problemas**:  
  - üö´ **L√≠mite de tokens**: Modelos como GPT-4 soportan ~8k-128k tokens. Un manual largo excede este l√≠mite.  
  - üí∏ **Costo alto**: Enviar 200 p√°ginas en cada consulta es caro (ej: 200k tokens cuestan ~$20 en GPT-4).  
  - üì¢ **Ruido contextual**: El LLM se distrae con texto irrelevante y puede dar respuestas incorrectas.

---

### **Opci√≥n 2: Usar RAG**
- **Proceso**:  
  1. **Divides el manual** en chunks peque√±os (ej: p√°rrafos).  
  2. **Indexas los chunks** en una base vectorial (ej: FAISS) usando embeddings.  
  3. Cuando llega una pregunta, **recuperas solo los chunks relevantes** (ej: 3 p√°rrafos sobre "reinicio").  
  4. **Env√≠as solo esos chunks + la pregunta** al LLM.  

- **Resultado**:  
  - ‚úÖ **Eficiencia**: Trabajas con 1% del texto original.  
  - üéØ **Precisi√≥n**: El LLM se enfoca en contexto relevante.  
  - üí∞ **Menor costo**: Pagas por ~1k tokens en lugar de 200k.

---

## Analog√≠a: Biblioteca vs. Libro Abierto
- **Sin RAG**:  
  Es como buscar una cita espec√≠fica en una biblioteca **sin √≠ndice ni cat√°logo**.  
  - Debes leer **todos los libros** cada vez que tienes una pregunta.  

- **Con RAG**:  
  Es como usar un **bibliotecario inteligente** que:  
  1. **Indexa todos los libros** por temas (embeddings).  
  2. Cuando preguntas, te trae **solo las p√°ginas relevantes**.  
  3. Luego, un experto (LLM) **resume la respuesta** usando esas p√°ginas.

---

## Comparaci√≥n Clave: RAG vs. "Pegar el Documento"

| **Aspecto**         | **Sin RAG**                                  | **Con RAG**                                  |
|----------------------|---------------------------------------------|---------------------------------------------|
| **L√≠mite de tokens** | Falla con documentos grandes.              | Usa solo chunks relevantes (escalable).     |
| **Costo**            | Muy alto (pagos por todo el documento).     | Bajo (pagos por chunks √∫tiles).             |
| **Velocidad**        | Lenta (procesa texto masivo).              | R√°pida (procesa solo lo relevante).         |
| **Precisi√≥n**        | Riesgo de alucinaciones por ruido.         | Respuestas enfocadas en contexto real.      |
| **Actualizaciones**  | Reenviar todo el documento cada vez.       | A√±adir/eliminar chunks din√°micamente.       |

---

## Ejemplo Real: Respuestas con y sin RAG
**Documento**:  
`"El protocolo X requiere 3 pasos: (1) Verificar conexi√≥n, (2) Ejecutar `sudo service restart`, (3) Validar logs en /var/logs."`

**Pregunta**:  
`"¬øC√≥mo reiniciar el servicio seg√∫n el protocolo X?"`

- **Sin RAG**:  
  ‚ùå `"Para reiniciar un servicio en Linux, usa `systemctl restart nombre-del-servicio`..."`  
  *(Alucinaci√≥n: el LLM no sabe del protocolo X).*

- **Con RAG**:  
  ‚úÖ `"Seg√∫n el protocolo X, ejecute: (1) Verifique la conexi√≥n, (2) Ejecute `sudo service restart`, (3) Valide logs en /var/logs."`

---

## ¬øCu√°ndo NO usar RAG?
- üìÑ **Documentos muy cortos** (ej: 1 p√°gina).  
- ‚ùì **Preguntas generales** que no requieren contexto espec√≠fico (ej: "¬øQu√© es un servidor?").  
- ‚öôÔ∏è **Recursos limitados**: Si no puedes gestionar una base vectorial.

---



## Ejercicio 3: Ejemplo de C√≥digo con Pinecone

1. Instalar las dependencias
``` bash
pip install fastapi uvicorn pinecone-client sentence-transformers python-dotenv pinecone
```

2. Generar una api key en Pinecone

Pinecone es una base de datos vectorial que permite almacenar y recuperar datos de manera eficiente. Pinecone convierte los documentos en vectores de alta dimensi√≥n (embeddings) utilizando un modelo de lenguaje (como el modelo de embeddings SentenceTransformer) y luego permite realizar b√∫squedas r√°pidas para recuperar los documentos m√°s similares a una consulta.

En el caso de RAG, Pinecone se utiliza para:

- Almacenar documentos: Convierte los documentos a embeddings y los guarda en un √≠ndice. Luego, cuando se realiza una consulta, puede buscar r√°pidamente los documentos m√°s relevantes basados en su similitud con la consulta.
- Recuperaci√≥n de documentos: Al recibir una consulta, Pinecone busca los documentos m√°s similares en el √≠ndice y devuelve los m√°s relevantes. Esta informaci√≥n se utiliza para crear un contexto relevante para la generaci√≥n de respuestas.

3. API con FastAPI (main.py)

```python
from fastapi import FastAPI, UploadFile, File
from pydantic import BaseModel
from pinecone import Pinecone
from sentence_transformers import SentenceTransformer
import os

from dotenv import load_dotenv
import os

# Cargar variables de entorno
load_dotenv()
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")

if not PINECONE_API_KEY:
    raise ValueError("Falta la API Key de Pinecone en el archivo .env")

INDEX_NAME = "rag-demo"

pc = Pinecone(api_key=PINECONE_API_KEY)
EMBEDDING_MODEL = SentenceTransformer('all-MiniLM-L6-v2')

# Crear √≠ndice si no existe
if INDEX_NAME not in [i["name"] for i in pc.list_indexes()]:
    pc.create_index(INDEX_NAME, dimension=EMBEDDING_MODEL.get_sentence_embedding_dimension())

index = pc.Index(INDEX_NAME)

app = FastAPI()

# Modelo para agregar documentos
class Document(BaseModel):
    doc_id: str
    text: str

# Modelo para consultas
class Query(BaseModel):
    text: str

@app.post("/add_document")
async def add_document(doc: Document):
    embedding = EMBEDDING_MODEL.encode(doc.text).tolist()
    index.upsert([(doc.doc_id, embedding, {"text": doc.text})])
    return {"message": "Documento agregado"}

@app.post("/query")
async def rag_query(query: Query):
    query_embedding = EMBEDDING_MODEL.encode(query.text).tolist()
    results = index.query(vector=query_embedding, top_k=3, include_metadata=True)

    context = " ".join([match["metadata"]["text"] for match in results["matches"] if "metadata" in match])

    return {"answer": f"Respuesta basada en: {context}", "context": context}
```

4. Ejecutar la API
```bash
uvicorn main:app --reload
```

Este endpoint est√° dise√±ado para agregar un documento de texto a Pinecone desde un JSON en el cual se env√≠a el texto del documento junto con un ID √∫nico.

¬øC√≥mo funciona?

Recibes un objeto JSON con:
- doc_id: El identificador √∫nico para el documento.
- text: El texto del documento que quieres agregar.

El texto se convierte en un embedding (vector num√©rico) usando el modelo de SentenceTransformer.
El documento con su doc_id, el embedding y el texto original se agrega al √≠ndice de Pinecone.

### Ejemplo de uso:

Si tienes un documento con informaci√≥n sobre un tema y quieres agregarlo a tu base de datos de Pinecone para que est√© disponible para consultas posteriores, usar√≠as este endpoint.


Tambi√©n se puede ingresar un documento desde un archivo .txt y agregar su contenido a Pinecone.

¬øC√≥mo funciona?

Recibes un archivo .txt.
El contenido del archivo se lee y se convierte a un embedding (vector num√©rico).
El documento se agrega al √≠ndice de Pinecone con el nombre del archivo como ID.
Ejemplo de uso:

Si tienes un archivo de texto (por ejemplo, un archivo con una receta o un art√≠culo) y quieres cargarlo en Pinecone para su posterior b√∫squeda, usar√≠as este endpoint.

C√≥digo de ejemplo:
```python
@app.on_event("startup")
async def startup_event():
    print("Cargando documentaci√≥n t√©cnica en Pinecone...")
    # Cargar documentos desde un directorio de archivos de texto o base de datos
    documents = load_documents_from_directory("docs/")  # Cargar todos los documentos del directorio
    embeddings = EMBEDDING_MODEL.encode(documents).tolist()
    index = pc.Index(INDEX_NAME)
    vectors = [{"id": str(i), "values": embedding, "metadata": {"text": doc}} 
               for i, (embedding, doc) in enumerate(zip(embeddings, documents))]
    index.upsert(vectors)
    print("Documentos cargados con √©xito.")
```

### ¬øPara qu√© sirve usar RAG con Pinecone?
1. Mejorar la capacidad de respuesta:
Los modelos de lenguaje como GPT-3 o T5 tienen una capacidad limitada de conocimiento en funci√≥n de su tama√±o y de los datos con los que fueron entrenados. RAG permite acceder a informaci√≥n m√°s actualizada o espec√≠fica al buscar en bases de datos externas, como Pinecone. 

Esto es √∫til cuando:
- El modelo necesita acceder a una gran cantidad de informaci√≥n no contenida en sus par√°metros.
La informaci√≥n es muy espec√≠fica o t√©cnica (por ejemplo, documentos cient√≠ficos, manuales, datos de productos, etc.).
- El conocimiento es din√°mico, como en el caso de noticias, documentos recientes, o bases de datos que cambian con frecuencia.

2. Sistema de preguntas y respuestas (QA):
Imagina que tienes un sistema de QA que responde a preguntas complejas sobre temas t√©cnicos, como documentaci√≥n de productos o informaci√≥n de soporte t√©cnico. Sin RAG, el modelo solo podr√≠a generar respuestas basadas en lo que ha aprendido durante su entrenamiento, lo que no siempre es suficiente ni preciso.

Con RAG y Pinecone, cuando un usuario hace una pregunta, el sistema primero recupera los documentos m√°s relevantes de la base de datos (Pinecone). Luego, el modelo usa esa informaci√≥n para generar una respuesta m√°s precisa y detallada. Por ejemplo, si alguien pregunta sobre un aspecto t√©cnico espec√≠fico de un producto, el sistema puede acceder a los manuales de usuario y gu√≠as de producto m√°s relevantes y generar una respuesta a partir de ah√≠.

3. Sistemas de recomendaci√≥n de contenido:
En aplicaciones donde los usuarios necesitan recomendaciones personalizadas (por ejemplo, recomendaci√≥n de art√≠culos o contenidos en l√≠nea), RAG puede mejorar el proceso de b√∫squeda de contenido. Pinecone puede almacenar los art√≠culos o p√°ginas de contenido, y el modelo puede usar las consultas de los usuarios para buscar el contenido m√°s relevante y generar recomendaciones de manera m√°s precisa.

4. Asistentes virtuales o chatbots avanzados:
Un asistente virtual o chatbot entrenado con RAG puede acceder a una base de datos espec√≠fica (por ejemplo, una base de datos de clientes, productos, o preguntas frecuentes) y recuperar informaci√≥n relevante para responder a las consultas de los usuarios. Esto permite que el asistente se base en datos actualizados y espec√≠ficos, mejorando la calidad de las respuestas.

5. B√∫squeda sem√°ntica:
Si tienes una base de datos grande con documentos, como art√≠culos cient√≠ficos, art√≠culos de blog, libros, o cualquier tipo de contenido textual, RAG y Pinecone pueden mejorar significativamente la b√∫squeda. La b√∫squeda tradicional basada en palabras clave no siempre es efectiva, ya que no tiene en cuenta la sem√°ntica o el significado real detr√°s de las palabras. Pinecone permite realizar b√∫squedas sem√°nticas donde los documentos m√°s relevantes se recuperan no solo por coincidencias exactas de palabras, sino por similitud sem√°ntica (es decir, significado).

